{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting click (from nltk)\n",
      "  Downloading click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting joblib (from nltk)\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting regex>=2021.8.3 (from nltk)\n",
      "  Downloading regex-2024.9.11-cp311-cp311-win_amd64.whl.metadata (41 kB)\n",
      "     ---------------------------------------- 0.0/41.5 kB ? eta -:--:--\n",
      "     --------- ------------------------------ 10.2/41.5 kB ? eta -:--:--\n",
      "     ---------------------------- --------- 30.7/41.5 kB 325.1 kB/s eta 0:00:01\n",
      "     -------------------------------------- 41.5/41.5 kB 333.1 kB/s eta 0:00:00\n",
      "Collecting tqdm (from nltk)\n",
      "  Downloading tqdm-4.66.5-py3-none-any.whl.metadata (57 kB)\n",
      "     ---------------------------------------- 0.0/57.6 kB ? eta -:--:--\n",
      "     ---------------------------------------- 57.6/57.6 kB 1.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: colorama in c:\\users\\dario\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from click->nltk) (0.4.6)\n",
      "Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ------- -------------------------------- 0.3/1.5 MB 5.9 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 0.8/1.5 MB 8.0 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 1.3/1.5 MB 9.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.5/1.5 MB 8.7 MB/s eta 0:00:00\n",
      "Downloading regex-2024.9.11-cp311-cp311-win_amd64.whl (274 kB)\n",
      "   ---------------------------------------- 0.0/274.0 kB ? eta -:--:--\n",
      "   ---------------------------------------- 274.0/274.0 kB 8.5 MB/s eta 0:00:00\n",
      "Downloading click-8.1.7-py3-none-any.whl (97 kB)\n",
      "   ---------------------------------------- 0.0/97.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 97.9/97.9 kB ? eta 0:00:00\n",
      "Downloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "   ---------------------------------------- 0.0/301.8 kB ? eta -:--:--\n",
      "   ---------------------------------------- 301.8/301.8 kB 9.4 MB/s eta 0:00:00\n",
      "Downloading tqdm-4.66.5-py3-none-any.whl (78 kB)\n",
      "   ---------------------------------------- 0.0/78.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 78.4/78.4 kB ? eta 0:00:00\n",
      "Installing collected packages: tqdm, regex, joblib, click, nltk\n",
      "Successfully installed click-8.1.7 joblib-1.4.2 nltk-3.9.1 regex-2024.9.11 tqdm-4.66.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.2\n",
      "[notice] To update, run: C:\\Users\\Dario\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import string\n",
    "from pathlib import Path\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "word_tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "# Define a custom sentence tokenizer for Albanian (splitting on sentence-ending punctuation)\n",
    "sentence_tokenizer = RegexpTokenizer(r'[^.!?]+[.!?]')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Domain</th>\n",
       "      <th>URL</th>\n",
       "      <th>Title</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Article</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Arkivi ditor: Jul 1, 2018</td>\n",
       "      <td>https://www.panorama.com.al/video-me-ke-lene-n...</td>\n",
       "      <td>VIDEO/ “Më ke lënë në rrugë, më ke ndarë me fa...</td>\n",
       "      <td>Rasti i kësaj të diele te “Shihemi në gjyq” mu...</td>\n",
       "      <td>Sado që flasim për kujdesin që duhet të bëjnë ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Arkivi ditor: Jul 1, 2018</td>\n",
       "      <td>https://www.panorama.com.al/50-000-vende-te-li...</td>\n",
       "      <td>50.000 vende të lira pune në Gjermani, kërkohe...</td>\n",
       "      <td>Ministri gjerman i Shëndetësisë Jens Spahn do ...</td>\n",
       "      <td>“Duhet të kërkojmë edhe jashtë vendit”Spahn po...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Arkivi ditor: Jul 1, 2018</td>\n",
       "      <td>https://www.panorama.com.al/video-kroacia-vuan...</td>\n",
       "      <td>VIDEO/ Kroacia vuan, por “nderohet” nga penall...</td>\n",
       "      <td>Kroacia mposhti Danimarkën me 11-metërsha, fal...</td>\n",
       "      <td>Kroacia provoi ta fitonte këtë takim brenda 90...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Arkivi ditor: Jul 1, 2018</td>\n",
       "      <td>https://www.panorama.com.al/legjenda-angleze-j...</td>\n",
       "      <td>Legjenda angleze: Januzaj të shpallet personaz...</td>\n",
       "      <td>Geri Linker ka një mendim ndryshe për Januzajn...</td>\n",
       "      <td>Për këtë ai jep arsyen se mesfushori shqiptar ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Arkivi ditor: Jul 1, 2018</td>\n",
       "      <td>https://www.panorama.com.al/daja-i-pergjigjet-...</td>\n",
       "      <td>Daja i përgjigjet Takajt: Ika nga Skënderbeu s...</td>\n",
       "      <td>Ilir Daja e ka cilësuar si një avetnurë të suk...</td>\n",
       "      <td>“Nuk ka braktisur asnjeri asgjë. Anija e quajt...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Domain  \\\n",
       "0  Arkivi ditor: Jul 1, 2018   \n",
       "1  Arkivi ditor: Jul 1, 2018   \n",
       "2  Arkivi ditor: Jul 1, 2018   \n",
       "3  Arkivi ditor: Jul 1, 2018   \n",
       "4  Arkivi ditor: Jul 1, 2018   \n",
       "\n",
       "                                                 URL  \\\n",
       "0  https://www.panorama.com.al/video-me-ke-lene-n...   \n",
       "1  https://www.panorama.com.al/50-000-vende-te-li...   \n",
       "2  https://www.panorama.com.al/video-kroacia-vuan...   \n",
       "3  https://www.panorama.com.al/legjenda-angleze-j...   \n",
       "4  https://www.panorama.com.al/daja-i-pergjigjet-...   \n",
       "\n",
       "                                               Title  \\\n",
       "0  VIDEO/ “Më ke lënë në rrugë, më ke ndarë me fa...   \n",
       "1  50.000 vende të lira pune në Gjermani, kërkohe...   \n",
       "2  VIDEO/ Kroacia vuan, por “nderohet” nga penall...   \n",
       "3  Legjenda angleze: Januzaj të shpallet personaz...   \n",
       "4  Daja i përgjigjet Takajt: Ika nga Skënderbeu s...   \n",
       "\n",
       "                                             Summary  \\\n",
       "0  Rasti i kësaj të diele te “Shihemi në gjyq” mu...   \n",
       "1  Ministri gjerman i Shëndetësisë Jens Spahn do ...   \n",
       "2  Kroacia mposhti Danimarkën me 11-metërsha, fal...   \n",
       "3  Geri Linker ka një mendim ndryshe për Januzajn...   \n",
       "4  Ilir Daja e ka cilësuar si një avetnurë të suk...   \n",
       "\n",
       "                                             Article  \n",
       "0  Sado që flasim për kujdesin që duhet të bëjnë ...  \n",
       "1  “Duhet të kërkojmë edhe jashtë vendit”Spahn po...  \n",
       "2  Kroacia provoi ta fitonte këtë takim brenda 90...  \n",
       "3  Për këtë ai jep arsyen se mesfushori shqiptar ...  \n",
       "4  “Nuk ka braktisur asnjeri asgjë. Anija e quajt...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "current_path = Path().absolute()\n",
    "# Load the original dataset\n",
    "df = pd.read_csv(current_path / 'news_panorama_jul_dec_2018.csv')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object, got 'float'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 28\u001b[0m\n\u001b[0;32m     25\u001b[0m title \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTitle\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Tokenize words and sentences\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m word_tokens \u001b[38;5;241m=\u001b[39m \u001b[43mword_tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43marticle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m sentence_tokens \u001b[38;5;241m=\u001b[39m sentence_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(article)\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# Calculate features for the article\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\nltk\\tokenize\\regexp.py:133\u001b[0m, in \u001b[0;36mRegexpTokenizer.tokenize\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    129\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_regexp\u001b[38;5;241m.\u001b[39msplit(text)\n\u001b[0;32m    131\u001b[0m \u001b[38;5;66;03m# If our regexp matches tokens, use re.findall:\u001b[39;00m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 133\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_regexp\u001b[38;5;241m.\u001b[39mfindall(text)\n",
      "\u001b[1;31mTypeError\u001b[0m: expected string or bytes-like object, got 'float'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Initialize lists to store analysis results\n",
    "word_counts = []\n",
    "sentence_counts = []\n",
    "paragraph_counts = []\n",
    "title_lengths = []\n",
    "summary_lengths = []\n",
    "avg_word_lengths = []\n",
    "reading_times = []\n",
    "summarization_ratios = []\n",
    "\n",
    "# Define a function to calculate average word length\n",
    "def average_word_length(text):\n",
    "    words = word_tokenizer.tokenize(text)\n",
    "    word_lengths = [len(word) for word in words if word not in string.punctuation]\n",
    "    return sum(word_lengths) / len(word_lengths) if len(word_lengths) > 0 else 0\n",
    "\n",
    "# Define a function to calculate estimated reading time (average 200 words/minute)\n",
    "def reading_time(word_count):\n",
    "    return round(word_count / 200, 2)  # Reading time in minutes\n",
    "\n",
    "# Iterate through each article in the dataset\n",
    "for index, row in df.iterrows():\n",
    "    article = row['Article']\n",
    "    summary = row['Summary']\n",
    "    title = row['Title']\n",
    "    \n",
    "    # Tokenize words and sentences\n",
    "    word_tokens = word_tokenizer.tokenize(article)\n",
    "    sentence_tokens = sentence_tokenizer.tokenize(article)\n",
    "    \n",
    "    # Calculate features for the article\n",
    "    word_count = len([word for word in word_tokens if word not in string.punctuation])\n",
    "    sentence_count = len(sentence_tokens)\n",
    "    paragraph_count = article.count('\\n') + 1  # Estimate paragraphs by counting new lines\n",
    "    avg_word_len = average_word_length(article)\n",
    "    read_time = reading_time(word_count)\n",
    "    \n",
    "    # Calculate title and summary word count\n",
    "    title_len = len(word_tokenizer.tokenize(title))\n",
    "    summary_len = len(word_tokenizer.tokenize(summary))\n",
    "    \n",
    "    # Calculate summarization ratio (summary words / article words)\n",
    "    summarization_ratio = summary_len / word_count if word_count > 0 else 0\n",
    "    \n",
    "    # Append calculated features to lists\n",
    "    word_counts.append(word_count)\n",
    "    sentence_counts.append(sentence_count)\n",
    "    paragraph_counts.append(paragraph_count)\n",
    "    avg_word_lengths.append(avg_word_len)\n",
    "    reading_times.append(read_time)\n",
    "    title_lengths.append(title_len)\n",
    "    summary_lengths.append(summary_len)\n",
    "    summarization_ratios.append(summarization_ratio)\n",
    "\n",
    "# Create a new DataFrame with the text analysis features\n",
    "analysis_df = pd.DataFrame({\n",
    "    'Title': df['Title'],\n",
    "    'Word_Count': word_counts,\n",
    "    'Sentence_Count': sentence_counts,\n",
    "    'Paragraph_Count': paragraph_counts,\n",
    "    'Avg_Word_Length': avg_word_lengths,\n",
    "    'Reading_Time_Minutes': reading_times,\n",
    "    'Title_Length': title_lengths,\n",
    "    'Summary_Length': summary_lengths,\n",
    "    'Summarization_Ratio': summarization_ratios\n",
    "})\n",
    "\n",
    "# Display the table in a readable format\n",
    "print(analysis_df.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory Path: c:\\Users\\Dario\\Desktop\\disertatie master\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "print(\"Directory Path:\", Path().absolute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Shqipëria', 'është', 'një', 'vend', 'i', 'vogël', 'në', 'Ballkan']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "# Define a custom tokenizer for Albanian (words including diacritics)\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "# Example Albanian text\n",
    "albanian_text = \"Shqipëria është një vend i vogël në Ballkan.\"\n",
    "\n",
    "# Tokenize the Albanian text\n",
    "tokens = tokenizer.tokenize(albanian_text)\n",
    "print(tokens)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
