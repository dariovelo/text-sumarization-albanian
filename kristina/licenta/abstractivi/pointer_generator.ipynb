{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)"
      ],
      "metadata": {
        "id": "3F0zUlfoXFiV"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "from csv import reader\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load the Albanian summarization dataset\n",
        "#dataset = load_dataset('csv', data_files={'train': 'train.csv', 'validation': 'validation.csv'})\n",
        "\n",
        "allNews = []\n",
        "with open('/content/final.csv', 'r') as read_obj:\n",
        "    # pass the file object to reader() to get the reader object\n",
        "    csv_reader = reader(read_obj)\n",
        "    # Iterate over each row in the csv using reader object\n",
        "    for row in csv_reader:\n",
        "        # row variable is a list that represents a row in csv\n",
        "        allNews.append(row)\n",
        "\n",
        "allNews.pop(0)\n",
        "articleDataset = []\n",
        "summaryDataset = []\n",
        "for news in allNews:\n",
        "  articleDataset.append(news[4])\n",
        "  summaryDataset.append(news[2] + news[3])\n",
        "\n",
        "# Split the data into training and test sets\n",
        "train_article, test_article = train_test_split(articleDataset, test_size=0.2, random_state=42)\n",
        "train_summary, test_summary = train_test_split(summaryDataset, test_size=0.2, random_state=42)\n",
        "train_article, validation_article, train_summary, validation_summary = train_test_split(train_article, train_summary, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "HYAifPpWYCM3"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "UTILS"
      ],
      "metadata": {
        "id": "SbMpbGJiWx56"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "class Vocab(object):\n",
        "    \"\"\"Class for storing the mapping between words and their corresponding index in the vocabulary\"\"\"\n",
        "    def __init__(self,tokenizer,max_size):\n",
        "        self._word_to_id = {}\n",
        "        self._id_to_word = {}\n",
        "        self._count = 0 # Total number of words in the Vocab\n",
        "        self._word_to_id['<PAD>'] = self._count\n",
        "        self._id_to_word[self._count] = '<PAD>'\n",
        "        self._count += 1\n",
        "        for _, word in tokenizer.index_word.items():\n",
        "            self._word_to_id[word] = self._count\n",
        "            self._id_to_word[self._count] = word\n",
        "            self._count += 1\n",
        "            if self._count >= max_size:\n",
        "                break\n",
        "\n",
        "    def word2id(self, word):\n",
        "        \"\"\"Returns the id (integer) of a word (string). Returns [UNK] id if word is OOV.\"\"\"\n",
        "        if word not in self._word_to_id:\n",
        "            return self._word_to_id['<UNK>']\n",
        "        return self._word_to_id[word]\n",
        "\n",
        "    def id2word(self, word_id):\n",
        "        \"\"\"Returns the word (string) corresponding to an id (integer).\"\"\"\n",
        "        if word_id not in self._id_to_word:\n",
        "            raise ValueError('Id not found in vocab: %d' % word_id)\n",
        "        return self._id_to_word[word_id]\n",
        "\n",
        "    def decode_seq(self,seq):\n",
        "        return \" \".join([self._id_to_word[idx] for idx in seq])\n",
        "\n",
        "    def size(self):\n",
        "        \"\"\"Returns the total size of the vocabulary\"\"\"\n",
        "        return self._count\n",
        "\n",
        "def text2seq(text,tokenizer,vocab):\n",
        "    \"\"\"Convert a string or list of strings to a sequence of vocabulary ids\n",
        "\n",
        "    Args:\n",
        "    text(string or list of strings): text input\n",
        "    tokenizer(object): tokenizer object\n",
        "    vocab(object): vocabulary object\n",
        "\n",
        "    Returns:\n",
        "    seqs_padded(int tensor): sequence of vocabulary ids padded with start and end token ids\n",
        "    \"\"\"\n",
        "    seqs = tokenizer.texts_to_sequences(text)\n",
        "    seqs = [[vocab._word_to_id['<s>']]+seq+[vocab._word_to_id['<\\s>']] for seq in seqs]\n",
        "    max_len_seq = max([len(s) for s in seqs])\n",
        "    seqs_padded = keras.preprocessing.sequence.pad_sequences(seqs, maxlen=max_len_seq, padding=\"post\")\n",
        "    return seqs_padded\n",
        "\n",
        "def greedy_search(encoder_input,model,vocab,max_len_sum = 30):\n",
        "    \"\"\"Function which returns a summary by always picking the highest probability option conditioned on the previous word\"\"\"\n",
        "    encoder_init_states = [tf.zeros((1, model.encoder.hidden_units)) for i in range(2)]\n",
        "    encoder_output, encoder_states = model.encoder(encoder_input,encoder_init_states)\n",
        "    decoder_state = encoder_states[0]\n",
        "\n",
        "    decoder_input_t = tf.ones(1)*vocab._word_to_id['<s>']\n",
        "    summary = [vocab._word_to_id['<s>']]\n",
        "    coverage_vector = tf.zeros((1,encoder_input.shape[1]))\n",
        "    while decoder_input_t[0].numpy()!=vocab._word_to_id['<\\s>'] and len(summary)<max_len_sum:\n",
        "        context_vector, attention_weights, coverage_vector = model.attention_model(decoder_state, encoder_output,coverage_vector)\n",
        "        p_vocab, decoder_state = model.decoder(tf.expand_dims(decoder_input_t,1),decoder_state,encoder_output,context_vector)\n",
        "        decoder_input_t = tf.argmax(p_vocab,axis=1)\n",
        "        decoder_word_idx = int(decoder_input_t[0].numpy())\n",
        "        summary.append(decoder_word_idx)\n",
        "    return summary\n",
        "\n",
        "def beam_search(encoder_input,model,vocab,beam_size=4,n_keep=4,max_len_sum=30):\n",
        "    encoder_init_states = [tf.zeros((1, model.encoder.hidden_units)) for i in range(2)]\n",
        "    encoder_output, encoder_states = model.encoder(encoder_input,encoder_init_states)\n",
        "    decoder_state = encoder_states[0]\n",
        "\n",
        "    coverage_vector = tf.zeros((1,encoder_input.shape[1]))\n",
        "    candidates = [[0,[vocab._word_to_id['<s>']],[decoder_state,coverage_vector]]]\n",
        "    not_terminated = True\n",
        "    longest_sum = 0\n",
        "    while not_terminated and longest_sum<max_len_sum:\n",
        "        new_candidates = []\n",
        "        for c_idx,cand in enumerate(candidates):\n",
        "            if cand[1][-1]!=vocab._word_to_id['<\\s>']:\n",
        "                decoder_input_t = tf.ones(1)*cand[1][-1]\n",
        "                decoder_state, coverage_vector = cand[2]\n",
        "                context_vector, attention_weights, coverage_vector = model.attention_model(decoder_state, encoder_output,coverage_vector)\n",
        "                p_vocab, decoder_state = model.decoder(tf.expand_dims(decoder_input_t,1),decoder_state,encoder_output,context_vector)\n",
        "                values,indicies = tf.math.top_k(p_vocab,k=beam_size)\n",
        "                for val,idx in zip(values.numpy()[0],indicies.numpy()[0]):\n",
        "                    new_idx_list = cand[1] + [idx]\n",
        "                    new_val = cand[0] + val\n",
        "                    new_candidates.append([new_val,new_idx_list,[decoder_state, coverage_vector]])\n",
        "            else:\n",
        "                new_candidates.append(cand)\n",
        "        candidates = sorted(new_candidates,key=lambda x:x[0]/len(x[1]),reverse=True)[:n_keep]\n",
        "        not_terminated = sum([cand[1][-1]!=vocab._word_to_id['<\\s>'] for cand in candidates])>0\n",
        "        longest_sum = max([len(cand[1]) for cand in candidates])\n",
        "\n",
        "    return candidates\n",
        "\n",
        "def masked_nll_loss(p_vocab,target):\n",
        "    \"\"\"Calculate negative log-likelihood loss and use mask to ignore padding\"\"\"\n",
        "    mask = tf.math.logical_not(tf.math.equal(target, 0))\n",
        "    loss = -p_vocab\n",
        "    mask = tf.cast(mask, dtype=loss.dtype)\n",
        "    loss *= mask\n",
        "    return loss\n",
        "\n",
        "def coverage_loss(attention_weights,coverage_vector,target):\n",
        "    mask = tf.math.logical_not(tf.math.equal(target, 0))\n",
        "    coverage_vector = tf.expand_dims(coverage_vector,axis=2)\n",
        "    ct_min = tf.reduce_min(tf.concat([attention_weights,coverage_vector],axis=2),axis=2)\n",
        "    cov_loss = tf.reduce_sum(ct_min,axis=1)\n",
        "    mask = tf.cast(mask, dtype=cov_loss.dtype)\n",
        "    cov_loss *= mask\n",
        "    return cov_loss"
      ],
      "metadata": {
        "id": "_7snZ3jcWuT2"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "MODELS"
      ],
      "metadata": {
        "id": "NM1Wq3b_W8Dm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "class Encoder(keras.Model):\n",
        "    \"\"\"Bi-directional GRU encoder\"\"\"\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_units,embedding_matrix):\n",
        "        super().__init__()\n",
        "\n",
        "        self.hidden_units = hidden_units\n",
        "        if embedding_matrix is not None:\n",
        "            self.embedding = keras.layers.Embedding(vocab_size, embedding_dim, weights=[embedding_matrix])\n",
        "        else:\n",
        "            self.embedding = keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        self.bi_gru = keras.layers.Bidirectional(keras.layers.GRU(\n",
        "                hidden_units,\n",
        "                return_sequences=True,\n",
        "                return_state=True,\n",
        "                recurrent_initializer='glorot_uniform',\n",
        "            ))\n",
        "\n",
        "    def call(self,encoder_input,encoder_states):\n",
        "        \"\"\"Forward pass of encoder\n",
        "        Args:\n",
        "        encoder_input(int tensor: (batch_size,seq_length) ): sequence(s) of vocabulary ids\n",
        "        encoder_states(list, len=2): encoder forward and backward state\n",
        "\n",
        "        Returns:\n",
        "        encoder_output(float tensor: (batch_size,seq_length,hidden_dim) ): encoded space of each sequence\n",
        "        encoder_states(list, len=2): updated encoder states\n",
        "        \"\"\"\n",
        "\n",
        "        encoder_emb = self.embedding(encoder_input)\n",
        "        encoder_output, state_fwd, state_back = self.bi_gru(encoder_emb,initial_state=encoder_states)\n",
        "        encoder_states = [state_fwd,state_back]\n",
        "\n",
        "        return encoder_output, encoder_states\n",
        "\n",
        "class BahdanauAttention(keras.Model):\n",
        "    \"\"\"Attention layer as described in: Neural Machine Translation by Jointly Learning to Align and Translate\"\"\"\n",
        "    def __init__(self, hidden_units,is_coverage=False):\n",
        "        super().__init__()\n",
        "\n",
        "        self.Wh = keras.layers.Dense(hidden_units) # weight matrix for encoder hidden state\n",
        "        self.Ws = keras.layers.Dense(hidden_units) # weight matrix for decoder state\n",
        "        self.V = keras.layers.Dense(1)\n",
        "        self.coverage = is_coverage\n",
        "        if self.coverage is False:\n",
        "            self.wc = keras.layers.Dense(1,kernel_initializer='zeros') # weight vector for coverage\n",
        "            self.wc.trainable = False\n",
        "        else:\n",
        "            self.wc = keras.layers.Dense(1)\n",
        "\n",
        "    def call(self, decoder_state, encoder_output,coverage_vector):\n",
        "        \"\"\"Forward pass of attention layer\n",
        "        Args:\n",
        "        decoder_state(float tensor: (batch_size,hidden_dim) )\n",
        "        encoder_output(float tensor: (batch_size,seq_length,hidden_dim) )\n",
        "        coverage_vector(float tensor: (batch_size,seq_length) )\n",
        "\n",
        "        Returns:\n",
        "        context_vector(float tensor: (batch_size,hidden_dim) )\n",
        "        attention_weights(float tensor: (batch_size,seq_length) )\n",
        "        coverage_vector(float tensor: (batch_size,seq_length) )\n",
        "        \"\"\"\n",
        "\n",
        "        # calculate attention scores\n",
        "        decoder_state = tf.expand_dims(decoder_state, 1)\n",
        "        coverage_vector = tf.expand_dims(coverage_vector, 1)\n",
        "        score = self.V(tf.nn.tanh(\n",
        "                        self.Wh(encoder_output) +\n",
        "                        self.Ws(decoder_state) +\n",
        "                        self.wc(coverage_vector)\n",
        "                        ))\n",
        "\n",
        "        attention_weights = tf.nn.softmax(score, axis=1)\n",
        "        coverage_vector = tf.squeeze(coverage_vector,1)\n",
        "        if self.coverage is True:\n",
        "          coverage_vector+=tf.squeeze(attention_weights)\n",
        "        context_vector = attention_weights * encoder_output\n",
        "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "\n",
        "        return context_vector, attention_weights, coverage_vector\n",
        "\n",
        "class Decoder(keras.Model):\n",
        "    \"\"\"Bi-directional GRU decoder with two dense layers in the end to model the vocabulary distribution\"\"\"\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_units,embedding_matrix):\n",
        "        super().__init__()\n",
        "\n",
        "        self.hidden_units = hidden_units\n",
        "        if embedding_matrix is not None:\n",
        "            self.embedding = keras.layers.Embedding(vocab_size, embedding_dim, weights=[embedding_matrix])\n",
        "        else:\n",
        "            self.embedding = keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        self.gru = keras.layers.GRU(\n",
        "            hidden_units,\n",
        "            return_sequences=True,\n",
        "            return_state=True,\n",
        "            recurrent_initializer='glorot_uniform',\n",
        "        )\n",
        "        self.W1 = keras.layers.Dense(hidden_units)\n",
        "        self.W2 = keras.layers.Dense(vocab_size)\n",
        "\n",
        "\n",
        "    def call(self, decoder_input, decoder_state, encoder_output,context_vector):\n",
        "        \"\"\"Forward pass of decoder\n",
        "\n",
        "        Args:\n",
        "        decoder_input(int tensor: (batch_size,1) )\n",
        "        decoder_state(float tensor: (batch_size,hidden_dim) )\n",
        "        encoder_output(float tensor: (batch_size,seq_length,hidden_dim) )\n",
        "        coverage_vector(float tensor: (batch_size,seq_length))\n",
        "\n",
        "        Returns:\n",
        "        p_vocab(float tensor: (batch_size,vocab_size) )\n",
        "        decoder_state(float tensor: (batch_size,hidden_dim) )\n",
        "        \"\"\"\n",
        "\n",
        "        decoder_emb = self.embedding(decoder_input) # (batch_size, seq_length, hidden_units)\n",
        "        decoder_output , decoder_state = self.gru(decoder_emb,initial_state=decoder_state)\n",
        "        concat_vector = tf.concat([context_vector,decoder_state], axis=-1)\n",
        "        concat_vector = tf.reshape(concat_vector, (-1, concat_vector.shape[1]))\n",
        "        p_vocab = tf.nn.log_softmax(self.W2(self.W1(concat_vector)))\n",
        "\n",
        "        return p_vocab, decoder_state"
      ],
      "metadata": {
        "id": "esnCNK2hW-TD"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "MAIN"
      ],
      "metadata": {
        "id": "Fa5sL68JXDHq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import argparse\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "class PointerGenerator:\n",
        "    def __init__(self):\n",
        "        #self.datapath = args.DATAPATH\n",
        "        #python your_script.py --DATAPATH custom_path --batch_size 32 --epochs 20 --edim 512 --hdim 256 --vdim 30000\n",
        "\n",
        "        self.batch_size = 32\n",
        "        self.n_epochs = 20\n",
        "        self.vocab_size = 30000\n",
        "        self.embedding_dim = 512\n",
        "        self.hidden_dim = 256\n",
        "        self.encoder = Encoder(self.vocab_size+2, self.embedding_dim, self.hidden_dim, embedding_matrix=None) #+2 on vocab size due to start and end token\n",
        "        self.attention_model = BahdanauAttention(self.hidden_dim,is_coverage=True)\n",
        "        self.decoder = Decoder(self.vocab_size+2, self.embedding_dim, self.hidden_dim, embedding_matrix=None)\n",
        "        self.optimizer = keras.optimizers.Adam()\n",
        "\n",
        "    @tf.function\n",
        "    def train_step(self,encoder_input, decoder_target):\n",
        "        \"\"\"Function which performs one training step\"\"\"\n",
        "        loss = tf.zeros(self.batch_size)\n",
        "        lambda_cov = 1\n",
        "        with tf.GradientTape() as tape:\n",
        "            encoder_init_states = [tf.zeros((self.batch_size, self.hidden_dim)) for i in range(2)]\n",
        "            encoder_output, encoder_states = self.encoder(encoder_input,encoder_init_states)\n",
        "            decoder_state = encoder_states[0] # alternative interpolate between forward and backward state\n",
        "            coverage_vector = tf.zeros((self.batch_size,encoder_input.shape[1]))\n",
        "            for t in range(decoder_target.shape[1]-1):\n",
        "                decoder_input_t = decoder_target[:,t]\n",
        "                decoder_target_t = decoder_target[:,t+1]\n",
        "                context_vector, attention_weights, coverage_vector = self.attention_model(decoder_state, encoder_output,coverage_vector)\n",
        "                p_vocab,decoder_state = self.decoder(tf.expand_dims(decoder_input_t,1),decoder_state,encoder_output,context_vector)\n",
        "                # for each batch get the probability of the target word at time t+1\n",
        "                p_vocab_list = []\n",
        "                for i in range(len(decoder_target_t)):\n",
        "                    p_vocab_list.append(p_vocab[i,decoder_target_t[i]])\n",
        "                p_vocab_target = tf.stack(p_vocab_list)\n",
        "                # calculate the loss at each time step t and add to current loss\n",
        "                loss += masked_nll_loss(p_vocab_target,decoder_target_t) + lambda_cov*coverage_loss(attention_weights,coverage_vector,decoder_target_t)\n",
        "\n",
        "            # get the non-padded length of each sequence in the batch\n",
        "            seq_len_mask = tf.cast(tf.math.logical_not(tf.math.equal(decoder_target, 0)),tf.float32)\n",
        "            batch_seq_len = tf.reduce_sum(seq_len_mask,axis=1)\n",
        "            batch_loss = tf.reduce_mean(loss/batch_seq_len)\n",
        "\n",
        "        variables = self.encoder.trainable_variables + self.decoder.trainable_variables\n",
        "        gradients = tape.gradient(batch_loss, variables)\n",
        "        self.optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "        return batch_loss\n",
        "\n",
        "\n",
        "    def train(self):\n",
        "        # load text data\n",
        "        body_data_train = train_article\n",
        "        target_data_train = train_summary\n",
        "        body_data_valid = validation_article\n",
        "        target_data_valid = validation_summary\n",
        "\n",
        "        # define vocabulary and tokenizer\n",
        "        tokenizer = keras.preprocessing.text.Tokenizer(num_words=self.vocab_size, oov_token='<UNK>')\n",
        "        tokenizer.fit_on_texts(body_data_train)\n",
        "        tokenizer.index_word[self.vocab_size] = '<s>' # add sentence start token\n",
        "        tokenizer.index_word[self.vocab_size+1] = '<\\s>' # add sentence end token\n",
        "        vocab = Vocab(tokenizer,self.vocab_size+2)\n",
        "\n",
        "        # create datasets\n",
        "        body_seqs_train = text2seq(body_data_train,tokenizer,vocab)\n",
        "        target_seqs_train = text2seq(target_data_train,tokenizer,vocab)\n",
        "        body_seqs_valid = text2seq(body_data_valid,tokenizer,vocab)\n",
        "        target_seqs_valid = text2seq(target_data_valid,tokenizer,vocab)\n",
        "        train_dataset = tf.data.Dataset.from_tensor_slices((body_seqs_train,target_seqs_train))\n",
        "        train_dataset = train_dataset.shuffle(len(body_seqs_train)).batch(self.batch_size, drop_remainder=True)\n",
        "        valid_dataset = tf.data.Dataset.from_tensor_slices((body_seqs_valid,target_seqs_valid))\n",
        "        valid_dataset = valid_dataset.shuffle(len(body_seqs_valid)).batch(1, drop_remainder=True)\n",
        "\n",
        "        # run one batch through model to initialize parameters\n",
        "        encoder_input, decoder_target = next(iter(train_dataset))\n",
        "        encoder_init_states = [tf.zeros((self.batch_size, self.hidden_dim)) for i in range(2)]\n",
        "        encoder_output, encoder_states = self.encoder(encoder_input,encoder_init_states)\n",
        "        decoder_state = encoder_states[0]\n",
        "        coverage_vector = tf.zeros((self.batch_size,encoder_input.shape[1]))\n",
        "        decoder_input_t = decoder_target[:,0]\n",
        "        context_vector, attention_weights, coverage_vector = self.attention_model(decoder_state, encoder_output,coverage_vector)\n",
        "        p_vocab,decoder_state = self.decoder(tf.expand_dims(decoder_input_t,1),decoder_state,encoder_output,context_vector)\n",
        "\n",
        "        # training loop\n",
        "        epoch_loss = keras.metrics.Mean()\n",
        "        for epoch in range(self.n_epochs):\n",
        "            epoch_loss.reset_states()\n",
        "\n",
        "            with tqdm(total=len(body_seqs_train) // self.batch_size) as batch_progress:\n",
        "                for batch, (encoder_input, decoder_target) in enumerate(train_dataset):\n",
        "                    batch_loss = self.train_step(encoder_input, decoder_target)\n",
        "                    epoch_loss(batch_loss)\n",
        "\n",
        "                    if (batch % 10) == 0:\n",
        "                        batch_progress.set_description(f'Epoch {epoch + 1}')\n",
        "                        batch_progress.set_postfix(Batch=batch, Loss=batch_loss.numpy())\n",
        "                        batch_progress.update()\n",
        "\n",
        "            self.eval()\n",
        "\n",
        "    def eval(self,vocab,valid_dataset):\n",
        "        encoder_input, decoder_target = next(iter(valid_dataset))\n",
        "        encoder_input_sum = tf.expand_dims(encoder_input[0,:],0)\n",
        "        greedy_summary = greedy_search(encoder_input_sum,self,vocab)\n",
        "        beam_summaries = beam_search(encoder_input_sum,self,vocab,beam_size=4,n_keep=4)\n",
        "        target_summary = [d for d in decoder_target.numpy()[0] if d!=0]\n",
        "\n",
        "        print(\"Greedy search:\"+vocab.decode_seq(greedy_summary))\n",
        "        print(\"Top 4 beam search: \\n\",\"\\n\".join([vocab.decode_seq(summary[1]) for summary in beam_summaries]))\n",
        "        print(\"Target:\"+vocab.decode_seq(target_summary))\n",
        "\n",
        "\n",
        "\n",
        "'''parser = argparse.ArgumentParser()\n",
        "parser.add_argument('--DATAPATH',default=\"title-gen-5m-tok\",type=str)\n",
        "parser.add_argument('--batch_size', default=16, type=int)\n",
        "parser.add_argument('--epochs', default=10, type=int)\n",
        "parser.add_argument('--edim', default=256, type=int)\n",
        "parser.add_argument('--hdim', default=128, type=int)\n",
        "parser.add_argument('--vdim', default=20000, type=int)\n",
        "args = parser.parse_args()'''\n",
        "\n",
        "pointgen = PointerGenerator()\n",
        "pointgen.train()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "25FRZuHeXESO",
        "outputId": "eeb7d518-191d-4f85-c908-6101fca27208"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/115 [00:00<?, ?it/s]"
          ]
        }
      ]
    }
  ]
}