{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets\n",
        "!pip install rouge\n",
        "!pip install nltk\n",
        "!pip install pycountry"
      ],
      "metadata": {
        "id": "BunamGOSg56b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import list_datasets\n",
        "from datasets import load_dataset\n",
        "\n",
        "\n",
        "print(list_datasets())\n",
        "dataset = load_dataset('cnn_dailymail', '3.0.0')\n",
        "\n",
        "# split data in train, validation, test\n",
        "train_article_set = dataset['train']['article']\n",
        "train_highlights_set = dataset['train']['highlights']\n",
        "\n",
        "validation_article_set = dataset['validation']['article']\n",
        "validation_highlights_set = dataset['validation']['highlights']\n",
        "\n",
        "test_article_set = dataset['test']['article']\n",
        "test_highlights_set = dataset['test']['highlights']"
      ],
      "metadata": {
        "id": "rRPozTXHhD-i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division, print_function, unicode_literals\n",
        "\n",
        "from sys import version_info\n",
        "\n",
        "\n",
        "PY3 = version_info[0] == 3\n",
        "\n",
        "\n",
        "if PY3:\n",
        "    bytes = bytes\n",
        "    unicode = str\n",
        "else:\n",
        "    bytes = str\n",
        "    unicode = unicode\n",
        "string_types = (bytes, unicode,)\n",
        "\n",
        "\n",
        "try:\n",
        "    from itertools import ifilterfalse as ffilter\n",
        "except ImportError:\n",
        "    from itertools import filterfalse as ffilter\n",
        "\n",
        "try:\n",
        "    from collections.abc import Sequence\n",
        "except ImportError:\n",
        "    from collections import Sequence\n",
        "\n",
        "\n",
        "def unicode_compatible(cls):\n",
        "    \"\"\"\n",
        "    Decorator for unicode compatible classes. Method ``__unicode__``\n",
        "    has to be implemented to work decorator as expected.\n",
        "    \"\"\"\n",
        "    if PY3:\n",
        "        cls.__str__ = cls.__unicode__\n",
        "        cls.__bytes__ = lambda self: self.__str__().encode(\"utf-8\")\n",
        "    else:\n",
        "        cls.__str__ = lambda self: self.__unicode__().encode(\"utf-8\")\n",
        "\n",
        "    return cls\n",
        "\n",
        "\n",
        "def to_string(object):\n",
        "    return to_unicode(object) if PY3 else to_bytes(object)\n",
        "\n",
        "\n",
        "def to_bytes(object):\n",
        "    if isinstance(object, bytes):\n",
        "        return object\n",
        "    elif isinstance(object, unicode):\n",
        "        return object.encode(\"utf-8\")\n",
        "    else:\n",
        "        # try encode instance to bytes\n",
        "        return instance_to_bytes(object)\n",
        "\n",
        "\n",
        "def to_unicode(object):\n",
        "    if isinstance(object, unicode):\n",
        "        return object\n",
        "    elif isinstance(object, bytes):\n",
        "        return object.decode(\"utf-8\")\n",
        "    else:\n",
        "        # try decode instance to unicode\n",
        "        return instance_to_unicode(object)\n",
        "\n",
        "\n",
        "def instance_to_bytes(instance):\n",
        "    if PY3:\n",
        "        if hasattr(instance, \"__bytes__\"):\n",
        "            return bytes(instance)\n",
        "        elif hasattr(instance, \"__str__\"):\n",
        "            return unicode(instance).encode(\"utf-8\")\n",
        "    else:\n",
        "        if hasattr(instance, \"__str__\"):\n",
        "            return bytes(instance)\n",
        "        elif hasattr(instance, \"__unicode__\"):\n",
        "            return unicode(instance).encode(\"utf-8\")\n",
        "\n",
        "    return to_bytes(repr(instance))\n",
        "\n",
        "\n",
        "def instance_to_unicode(instance):\n",
        "    if PY3:\n",
        "        if hasattr(instance, \"__str__\"):\n",
        "            return unicode(instance)\n",
        "        elif hasattr(instance, \"__bytes__\"):\n",
        "            return bytes(instance).decode(\"utf-8\")\n",
        "    else:\n",
        "        if hasattr(instance, \"__unicode__\"):\n",
        "            return unicode(instance)\n",
        "        elif hasattr(instance, \"__str__\"):\n",
        "            return bytes(instance).decode(\"utf-8\")\n",
        "\n",
        "    return to_unicode(repr(instance))"
      ],
      "metadata": {
        "id": "oXSWWoJEskcA"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division, print_function, unicode_literals\n",
        "\n",
        "import math\n",
        "\n",
        "from collections import Counter\n",
        "from pprint import pformat\n",
        "\n",
        "class TfDocumentModel(object):\n",
        "    \"\"\"Term-Frequency document model (term = word).\"\"\"\n",
        "    def __init__(self, words, tokenizer=None):\n",
        "        if isinstance(words, string_types) and tokenizer is None:\n",
        "            raise ValueError(\n",
        "                \"Tokenizer has to be given if ``words`` is not a sequence.\")\n",
        "        elif isinstance(words, string_types):\n",
        "            words = tokenizer.to_words(to_unicode(words))\n",
        "        elif not isinstance(words, Sequence):\n",
        "            raise ValueError(\n",
        "                \"Parameter ``words`` has to be sequence or string with tokenizer given.\")\n",
        "\n",
        "        self._terms = Counter(map(unicode.lower, words))\n",
        "        self._max_frequency = max(self._terms.values()) if self._terms else 1\n",
        "\n",
        "    @property\n",
        "    def magnitude(self):\n",
        "        \"\"\"\n",
        "        Lenght/norm/magnitude of vector representation of document.\n",
        "        This is usually denoted by ||d||.\n",
        "        \"\"\"\n",
        "        return math.sqrt(sum(t**2 for t in self._terms.values()))\n",
        "\n",
        "    @property\n",
        "    def terms(self):\n",
        "        return self._terms.keys()\n",
        "\n",
        "    def most_frequent_terms(self, count=0):\n",
        "        \"\"\"\n",
        "        Returns ``count`` of terms sorted by their frequency\n",
        "        in descending order.\n",
        "        :parameter int count:\n",
        "            Max. number of returned terms. Value 0 means no limit (default).\n",
        "        \"\"\"\n",
        "        # sort terms by number of occurrences in descending order\n",
        "        terms = sorted(self._terms.items(), key=lambda i: -i[1])\n",
        "\n",
        "        terms = tuple(i[0] for i in terms)\n",
        "        if count == 0:\n",
        "            return terms\n",
        "        elif count > 0:\n",
        "            return terms[:count]\n",
        "        else:\n",
        "            raise ValueError(\n",
        "                \"Only non-negative values are allowed for count of terms.\")\n",
        "\n",
        "    def term_frequency(self, term):\n",
        "        \"\"\"\n",
        "        Returns frequency of term in document.\n",
        "        :returns int:\n",
        "            Returns count of words in document.\n",
        "        \"\"\"\n",
        "        return self._terms.get(term, 0)\n",
        "\n",
        "    def normalized_term_frequency(self, term, smooth=0.0):\n",
        "        \"\"\"\n",
        "        Returns normalized frequency of term in document.\n",
        "        http://nlp.stanford.edu/IR-book/html/htmledition/maximum-tf-normalization-1.html\n",
        "        :parameter float smooth:\n",
        "            0.0 <= smooth <= 1.0, generally set to 0.4, although some\n",
        "            early work used the value 0.5. The term is a smoothing term\n",
        "            whose role is to damp the contribution of the second term.\n",
        "            It may be viewed as a scaling down of TF by the largest TF\n",
        "            value in document.\n",
        "        :returns float:\n",
        "            0.0 <= frequency <= 1.0, where 0 means no occurrence in document\n",
        "            and 1 the most frequent term in document.\n",
        "        \"\"\"\n",
        "        frequency = self.term_frequency(term) / self._max_frequency\n",
        "        return smooth + (1.0 - smooth)*frequency\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"<TfDocumentModel %s>\" % pformat(self._terms)"
      ],
      "metadata": {
        "id": "iOh7dXRRsUTX"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Utils\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division, print_function, unicode_literals\n",
        "\n",
        "import sys\n",
        "import requests\n",
        "import pkgutil\n",
        "\n",
        "from functools import wraps\n",
        "from contextlib import closing\n",
        "from os.path import dirname, abspath, join\n",
        "\n",
        "from pycountry import languages\n",
        "\n",
        "_HTTP_HEADERS = {\n",
        "    \"User-Agent\": \"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:109.0) Gecko/20100101 Firefox/109.0\",\n",
        "    # \"User-Agent\": \"Sumy (Automatic text summarizer) Version/%s\" % __version__,\n",
        "}\n",
        "\n",
        "\n",
        "def normalize_language(language):\n",
        "    for lookup_key in (\"alpha_2\", \"alpha_3\"):\n",
        "        try:\n",
        "            lang = languages.get(**{lookup_key: language})\n",
        "            if lang:\n",
        "                language = lang.name.lower()\n",
        "        except KeyError:\n",
        "            pass\n",
        "\n",
        "    return language\n",
        "\n",
        "\n",
        "def fetch_url(url, timeout=(3.05, 30)):\n",
        "    with closing(requests.get(url, headers=_HTTP_HEADERS, timeout=timeout)) as response:\n",
        "        response.raise_for_status()\n",
        "        return response.content\n",
        "\n",
        "\n",
        "def cached_property(getter):\n",
        "    \"\"\"\n",
        "    Decorator that converts a method into memoized property.\n",
        "    The decorator works as expected only for classes with\n",
        "    attribute '__dict__' and immutable properties.\n",
        "    \"\"\"\n",
        "    @wraps(getter)\n",
        "    def decorator(self):\n",
        "        key = \"_cached_property_\" + getter.__name__\n",
        "\n",
        "        if not hasattr(self, key):\n",
        "            setattr(self, key, getter(self))\n",
        "\n",
        "        return getattr(self, key)\n",
        "\n",
        "    return property(decorator)\n",
        "\n",
        "\n",
        "def expand_resource_path(path):\n",
        "    directory = dirname(sys.modules[\"sumy\"].__file__)\n",
        "    directory = abspath(directory)\n",
        "    return join(directory, to_string(\"data\"), to_string(path))\n",
        "\n",
        "\n",
        "def get_stop_words(language):\n",
        "    language = normalize_language(language)\n",
        "    try:\n",
        "        stopwords_data = pkgutil.get_data(\"sumy\", \"data/stopwords/%s.txt\" % language)\n",
        "    except IOError:\n",
        "        raise LookupError(\"Stop-words are not available for language %s.\" % language)\n",
        "    return parse_stop_words(stopwords_data)\n",
        "\n",
        "\n",
        "def read_stop_words(filename):\n",
        "    with open(filename, \"rb\") as open_file:\n",
        "        return parse_stop_words(open_file.read())\n",
        "\n",
        "\n",
        "def parse_stop_words(data):\n",
        "    return frozenset(w.rstrip() for w in to_unicode(data).splitlines() if w)\n",
        "\n",
        "\n",
        "class ItemsCount(object):\n",
        "    def __init__(self, value):\n",
        "        self._value = value\n",
        "\n",
        "    def __call__(self, sequence):\n",
        "        if isinstance(self._value, string_types):\n",
        "            if self._value.endswith(\"%\"):\n",
        "                total_count = len(sequence)\n",
        "                percentage = int(self._value[:-1])\n",
        "                # at least one sentence should be chosen\n",
        "                count = max(1, total_count*percentage // 100)\n",
        "                return sequence[:count]\n",
        "            else:\n",
        "                return sequence[:int(self._value)]\n",
        "        elif isinstance(self._value, (int, float)):\n",
        "            return sequence[:int(self._value)]\n",
        "        else:\n",
        "            ValueError(\"Unsuported value of items count '%s'.\" % self._value)\n",
        "\n",
        "    def __repr__(self):\n",
        "        return to_string(\"<ItemsCount: %r>\" % self._value)"
      ],
      "metadata": {
        "id": "Dgd0LxU6s5Up"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import namedtuple\n",
        "from operator import attrgetter\n",
        "\n",
        "def null_stemmer(object):\n",
        "    \"\"\"Converts given object to unicode with lower letters.\"\"\"\n",
        "    return to_unicode(object).lower()\n",
        "\n",
        "SentenceInfo = namedtuple(\"SentenceInfo\", (\"sentence\", \"order\", \"rating\",))\n",
        "\n",
        "\n",
        "class AbstractSummarizer(object):\n",
        "    def __init__(self, stemmer=null_stemmer):\n",
        "        if not callable(stemmer):\n",
        "            raise ValueError(\"Stemmer has to be a callable object\")\n",
        "\n",
        "        self._stemmer = stemmer\n",
        "\n",
        "    def __call__(self, document, sentences_count):\n",
        "        raise NotImplementedError(\"This method should be overriden in subclass\")\n",
        "\n",
        "    def stem_word(self, word):\n",
        "        return self._stemmer(self.normalize_word(word))\n",
        "\n",
        "    @staticmethod\n",
        "    def normalize_word(word):\n",
        "        return to_unicode(word).lower()\n",
        "\n",
        "    @staticmethod\n",
        "    def _get_best_sentences(sentences, count, rating, *args, **kwargs):\n",
        "        rate = rating\n",
        "        if isinstance(rating, dict):\n",
        "            assert not args and not kwargs\n",
        "            def rate(s): return rating[s]\n",
        "\n",
        "        infos = (SentenceInfo(s, o, rate(s, *args, **kwargs))\n",
        "            for o, s in enumerate(sentences))\n",
        "\n",
        "        # sort sentences by rating in descending order\n",
        "        infos = sorted(infos, key=attrgetter(\"rating\"), reverse=True)\n",
        "        # get `count` first best rated sentences\n",
        "        if not callable(count):\n",
        "            count = ItemsCount(count)\n",
        "        infos = count(infos)\n",
        "        # sort sentences by their order in document\n",
        "        infos = sorted(infos, key=attrgetter(\"order\"))\n",
        "\n",
        "        return tuple(i.sentence for i in infos)"
      ],
      "metadata": {
        "id": "ubMinxRbsx37"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division, print_function, unicode_literals\n",
        "\n",
        "\n",
        "class LuhnSummarizer(AbstractSummarizer):\n",
        "    max_gap_size = 4\n",
        "    # TODO: better recognition of significant words (automatic)\n",
        "    significant_percentage = 1\n",
        "    _stop_words = frozenset()\n",
        "\n",
        "    @property\n",
        "    def stop_words(self):\n",
        "        return self._stop_words\n",
        "\n",
        "    @stop_words.setter\n",
        "    def stop_words(self, words):\n",
        "        self._stop_words = frozenset(map(self.normalize_word, words))\n",
        "\n",
        "    def __call__(self, document, sentences_count):\n",
        "        words = self._get_significant_words(nltk.word_tokenize(sentence) for sentence in document)\n",
        "        return self._get_best_sentences(nltk.sent_tokenize(document),\n",
        "            sentences_count, self.rate_sentence, words)\n",
        "\n",
        "    def _get_significant_words(self, words):\n",
        "        words = map(self.normalize_word, words)\n",
        "        words = tuple(self.stem_word(w) for w in words if w not in self._stop_words)\n",
        "\n",
        "        model = TfDocumentModel(words)\n",
        "\n",
        "        # take only best `significant_percentage` % words\n",
        "        best_words_count = int(len(words) * self.significant_percentage)\n",
        "        words = model.most_frequent_terms(best_words_count)\n",
        "\n",
        "        # take only words contained multiple times in document\n",
        "        return tuple(t for t in words if model.term_frequency(t) > 1)\n",
        "\n",
        "    def rate_sentence(self, sentence, significant_stems):\n",
        "        ratings = self._get_chunk_ratings(sentence, significant_stems)\n",
        "        return max(ratings) if ratings else 0\n",
        "\n",
        "    def _get_chunk_ratings(self, sentence, significant_stems):\n",
        "        chunks = []\n",
        "        NONSIGNIFICANT_CHUNK = [0]*self.max_gap_size\n",
        "\n",
        "        in_chunk = False\n",
        "        for order, word in enumerate(nltk.word_tokenize(sentence)):\n",
        "            stem = self.stem_word(word)\n",
        "            # new chunk\n",
        "            if stem in significant_stems and not in_chunk:\n",
        "                in_chunk = True\n",
        "                chunks.append([1])\n",
        "            # append word to chunk\n",
        "            elif in_chunk:\n",
        "                is_significant_word = int(stem in significant_stems)\n",
        "                chunks[-1].append(is_significant_word)\n",
        "\n",
        "            # end of chunk\n",
        "            if chunks and chunks[-1][-self.max_gap_size:] == NONSIGNIFICANT_CHUNK:\n",
        "                in_chunk = False\n",
        "\n",
        "        return tuple(map(self._get_chunk_rating, chunks))\n",
        "\n",
        "    def _get_chunk_rating(self, chunk):\n",
        "        chunk = self.__remove_trailing_zeros(chunk)\n",
        "        words_count = len(chunk)\n",
        "        assert words_count > 0\n",
        "\n",
        "        significant_words = sum(chunk)\n",
        "        if significant_words == 1:\n",
        "            return 0\n",
        "        else:\n",
        "            return significant_words**2 / words_count\n",
        "\n",
        "    def __remove_trailing_zeros(self, collection):\n",
        "        \"\"\"Removes trailing zeroes from indexable collection of numbers\"\"\"\n",
        "        index = len(collection) - 1\n",
        "        while index >= 0 and collection[index] == 0:\n",
        "            index -= 1\n",
        "\n",
        "        return collection[:index + 1]\n"
      ],
      "metadata": {
        "id": "a5zeMpPjsIt0"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "summarizer = LuhnSummarizer()\n",
        "predicted_summary = []\n",
        "for article in train_article_set[:30000]:\n",
        "  summary = summarizer(article, 2)\n",
        "  summ = \"\"\n",
        "  for s in summary:\n",
        "    summ += s\n",
        "  predicted_summary.append(summ)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L8qa9e5JsTZL",
        "outputId": "ac846ebe-ddfc-47ec-82c2-ee9850376db6"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from rouge import Rouge\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "import numpy as np\n",
        "\n",
        "# Compute the ROUGE metrics\n",
        "rouge = Rouge()\n",
        "\n",
        "rouge_scores = rouge.get_scores(predicted_summary, train_highlights_set[:30000], avg=True)\n",
        "\n",
        "print(\"ROUGE scores:\")\n",
        "print(rouge_scores)\n",
        "\n",
        "score_1 = round(rouge_scores['rouge-1']['f'], 2)    \n",
        "score_2 = round(rouge_scores['rouge-2']['f'], 2)    \n",
        "score_L = round(rouge_scores['rouge-l']['f'], 2)    \n",
        "print(\"rouge1:\", score_1, \"| rouge2:\", score_2, \"| rougeL:\",\n",
        "         score_2, \"--> avg rouge:\", round(np.mean(\n",
        "         [score_1,score_2,score_L]), 2))\n",
        "# Compute the BLEU metrics\n",
        "bleu_scores = corpus_bleu([[summary] for summary in predicted_summary], train_highlights_set[:30000])\n",
        "\n",
        "print(\"BLEU score:\")\n",
        "print(bleu_scores)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FTPVaVR2ynnD",
        "outputId": "8c0f5a23-6386-4b97-8c22-bb8fda658b34"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROUGE scores:\n",
            "{'rouge-1': {'r': 0.30795307792340226, 'p': 0.2602783144833186, 'f': 0.27518810135341126}, 'rouge-2': {'r': 0.11018673345219916, 'p': 0.08891481138779438, 'f': 0.09565356937089269}, 'rouge-l': {'r': 0.28090255543678955, 'p': 0.2376418754879394, 'f': 0.2510840490332295}}\n",
            "rouge1: 0.28 | rouge2: 0.1 | rougeL: 0.1 --> avg rouge: 0.21\n",
            "BLEU score:\n",
            "0.39533439178146246\n"
          ]
        }
      ]
    }
  ]
}