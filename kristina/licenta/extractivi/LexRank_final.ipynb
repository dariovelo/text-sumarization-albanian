{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C2So62pZ8INa"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "!pip install datasets\n",
        "!pip install lexrank\n",
        "!pip install rouge\n",
        "!pip install sumy\n",
        "!pip install nltk\n",
        "!pip install gensim\n",
        "\n",
        "from datasets import list_datasets\n",
        "from datasets import load_dataset\n",
        "\n",
        "#cnn mail daily"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m47R1kB58mI0"
      },
      "outputs": [],
      "source": [
        "dataset = load_dataset('cnn_dailymail', '3.0.0')\n",
        "\n",
        "# split data in train, validation, test\n",
        "train_article_set = dataset['train']['article']\n",
        "train_highlights_set = dataset['train']['highlights']\n",
        "\n",
        "validation_article_set = dataset['validation']['article']\n",
        "validation_highlights_set = dataset['validation']['highlights']\n",
        "\n",
        "test_article_set = dataset['test']['article']\n",
        "test_highlights_set = dataset['test']['highlights']\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "from collections import Counter, defaultdict\n",
        "import numpy as np\n",
        "import regex\n",
        "from urlextract import URLExtract\n",
        "from scipy.sparse.csgraph import connected_components\n",
        "import nltk\n",
        "from sumy.nlp.stemmers import Stemmer\n",
        "from rouge import Rouge\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('stopwords')\n",
        "\n",
        "EMAIL_REGEX = regex.compile(\n",
        "    r'[\\p{L}0-9]+[\\p{L}0-9_.+-]*[\\p{L}0-9_+-]+@[\\p{L}0-9]+[\\p{L}0-9.-]*\\.\\p{L}+'  # noqa\n",
        ")\n",
        "PUNCTUATION_SIGNS = set('.,;:¡!¿?…⋯&‹›«»\\\"“”[]()⟨⟩}{/|\\\\')\n",
        "\n",
        "url_extractor = URLExtract()\n",
        "\n",
        "\n",
        "def clean_text(text, allowed_chars='- '):\n",
        "    text = ' '.join(text.lower().split())\n",
        "    text = ''.join(ch for ch in text if ch.isalnum() or ch in allowed_chars)\n",
        "\n",
        "    return text\n",
        "\n",
        "\n",
        "def contains_letters(word):\n",
        "    return any(ch.isalpha() for ch in word)\n",
        "\n",
        "\n",
        "def contains_numbers(word):\n",
        "    return any(ch.isdigit() for ch in word)\n",
        "\n",
        "\n",
        "def filter_words(words, stopwords, keep_numbers=False):\n",
        "    if keep_numbers:\n",
        "        words = [\n",
        "            word for word in words\n",
        "            if (contains_letters(word) or contains_numbers(word))\n",
        "            and word not in stopwords\n",
        "        ]\n",
        "\n",
        "    else:\n",
        "        words = [\n",
        "            word for word in words\n",
        "            if contains_letters(word) and not contains_numbers(word)\n",
        "            and word not in stopwords\n",
        "        ]\n",
        "\n",
        "    return words\n",
        "\n",
        "\n",
        "def separate_punctuation(text):\n",
        "    text_punctuation = set(text) & PUNCTUATION_SIGNS\n",
        "\n",
        "    for ch in text_punctuation:\n",
        "        text = text.replace(ch, ' ' + ch + ' ')\n",
        "\n",
        "    return text\n",
        "\n",
        "def tokenize(\n",
        "    text,\n",
        "    stopwords,\n",
        "    keep_numbers=False,\n",
        "    keep_emails=False,\n",
        "    keep_urls=False,\n",
        "):\n",
        "    tokens = []\n",
        "\n",
        "    for word in text.split():\n",
        "        emails = EMAIL_REGEX.findall(word)\n",
        "\n",
        "        if emails:\n",
        "            if keep_emails:\n",
        "                tokens.append(emails[0])\n",
        "\n",
        "            continue\n",
        "\n",
        "        urls = url_extractor.find_urls(word, only_unique=True)\n",
        "\n",
        "        if urls:\n",
        "            if keep_urls:\n",
        "                tokens.append(urls[0].lower())\n",
        "\n",
        "            continue\n",
        "\n",
        "        cleaned = clean_text(separate_punctuation(word)).split()\n",
        "        cleaned = filter_words(cleaned, stopwords, keep_numbers=keep_numbers)\n",
        "\n",
        "        tokens.extend(cleaned)\n",
        "\n",
        "    return tokens\n",
        "\n",
        "def _power_method(transition_matrix, increase_power=True):\n",
        "    eigenvector = np.ones(len(transition_matrix))\n",
        "\n",
        "    if len(eigenvector) == 1:\n",
        "        return eigenvector\n",
        "\n",
        "    transition = transition_matrix.transpose()\n",
        "\n",
        "    while True:\n",
        "        eigenvector_next = np.dot(transition, eigenvector)\n",
        "\n",
        "        if np.allclose(eigenvector_next, eigenvector):\n",
        "            return eigenvector_next\n",
        "\n",
        "        eigenvector = eigenvector_next\n",
        "\n",
        "        if increase_power:\n",
        "            transition = np.dot(transition, transition)\n",
        "\n",
        "\n",
        "def connected_nodes(matrix):\n",
        "    _, labels = connected_components(matrix)\n",
        "\n",
        "    groups = []\n",
        "\n",
        "    for tag in np.unique(labels):\n",
        "        group = np.where(labels == tag)[0]\n",
        "        groups.append(group)\n",
        "\n",
        "    return groups\n",
        "def create_markov_matrix(weights_matrix):\n",
        "    n_1, n_2 = weights_matrix.shape\n",
        "    if n_1 != n_2:\n",
        "        raise ValueError('\\'weights_matrix\\' should be square')\n",
        "\n",
        "    row_sum = weights_matrix.sum(axis=1, keepdims=True)\n",
        "\n",
        "    return weights_matrix / row_sum\n",
        "\n",
        "def create_markov_matrix_discrete(weights_matrix, threshold):\n",
        "    discrete_weights_matrix = np.zeros(weights_matrix.shape)\n",
        "    ixs = np.where(weights_matrix >= threshold)\n",
        "    discrete_weights_matrix[ixs] = 1\n",
        "\n",
        "    return create_markov_matrix(discrete_weights_matrix)\n",
        "\n",
        "def graph_nodes_clusters(transition_matrix, increase_power=True):\n",
        "    clusters = connected_nodes(transition_matrix)\n",
        "    clusters.sort(key=len, reverse=True)\n",
        "\n",
        "    centroid_scores = []\n",
        "\n",
        "    for group in clusters:\n",
        "        t_matrix = transition_matrix[np.ix_(group, group)]\n",
        "        eigenvector = _power_method(t_matrix, increase_power=increase_power)\n",
        "        centroid_scores.append(eigenvector / len(group))\n",
        "\n",
        "    return clusters, centroid_scores\n",
        "\n",
        "def stationary_distribution(\n",
        "    transition_matrix,\n",
        "    increase_power=True,\n",
        "    normalized=True,\n",
        "):\n",
        "    n_1, n_2 = transition_matrix.shape\n",
        "    if n_1 != n_2:\n",
        "        raise ValueError('\\'transition_matrix\\' should be square')\n",
        "\n",
        "    distribution = np.zeros(n_1)\n",
        "\n",
        "    grouped_indices = connected_nodes(transition_matrix)\n",
        "\n",
        "    for group in grouped_indices:\n",
        "        t_matrix = transition_matrix[np.ix_(group, group)]\n",
        "        eigenvector = _power_method(t_matrix, increase_power=increase_power)\n",
        "        distribution[group] = eigenvector\n",
        "\n",
        "    if normalized:\n",
        "        distribution /= n_1\n",
        "\n",
        "    return distribution\n",
        "\n",
        "def degree_centrality_scores(\n",
        "    similarity_matrix,\n",
        "    threshold=None,\n",
        "    increase_power=True,\n",
        "):\n",
        "    if not (\n",
        "        threshold is None\n",
        "        or isinstance(threshold, float)\n",
        "        and 0 <= threshold < 1\n",
        "    ):\n",
        "        raise ValueError(\n",
        "            '\\'threshold\\' should be a floating-point number '\n",
        "            'from the interval [0, 1) or None',\n",
        "        )\n",
        "\n",
        "    if threshold is None:\n",
        "        markov_matrix = create_markov_matrix(similarity_matrix)\n",
        "\n",
        "    else:\n",
        "        markov_matrix = create_markov_matrix_discrete(\n",
        "            similarity_matrix,\n",
        "            threshold,\n",
        "        )\n",
        "\n",
        "    scores = stationary_distribution(\n",
        "        markov_matrix,\n",
        "        increase_power=increase_power,\n",
        "        normalized=False,\n",
        "    )\n",
        "\n",
        "    return scores\n",
        "\n",
        "\n",
        "class LexRank:\n",
        "    def __init__(\n",
        "        self,\n",
        "        documents,\n",
        "        stopwords=None,\n",
        "        keep_numbers=False,\n",
        "        keep_emails=False,\n",
        "        keep_urls=False,\n",
        "        include_new_words=True,\n",
        "    ):\n",
        "        if stopwords is None:\n",
        "            self.stopwords = set()\n",
        "        else:\n",
        "            self.stopwords = stopwords\n",
        "\n",
        "        self.keep_numbers = keep_numbers\n",
        "        self.keep_emails = keep_emails\n",
        "        self.keep_urls = keep_urls\n",
        "        self.include_new_words = include_new_words\n",
        "\n",
        "        self.idf_score = self._calculate_idf(documents)\n",
        "\n",
        "    def get_summary(\n",
        "        self,\n",
        "        sentences,\n",
        "        summary_size,\n",
        "        threshold,\n",
        "        fast_power_method=True,\n",
        "    ):\n",
        "        if not isinstance(summary_size, int) or summary_size < 1:\n",
        "            raise ValueError('\\'summary_size\\' should be a positive integer')\n",
        "\n",
        "        lex_scores = self.rank_sentences(\n",
        "            sentences,\n",
        "            threshold=threshold,\n",
        "            fast_power_method=fast_power_method,\n",
        "        )\n",
        "\n",
        "        sorted_ix = np.argsort(lex_scores)[::-1]\n",
        "        summary = [sentences[i] for i in sorted_ix[:summary_size]]\n",
        "\n",
        "        return summary\n",
        "\n",
        "    def rank_sentences(\n",
        "        self,\n",
        "        sentences,\n",
        "        threshold=.03,\n",
        "        fast_power_method=True,\n",
        "    ):\n",
        "        tf_scores = [\n",
        "            Counter(self.tokenize_sentence(sentence)) for sentence in sentences\n",
        "        ]\n",
        "\n",
        "        similarity_matrix = self._calculate_similarity_matrix(tf_scores)\n",
        "\n",
        "        scores = degree_centrality_scores(\n",
        "            similarity_matrix,\n",
        "            threshold=threshold,\n",
        "            increase_power=fast_power_method,\n",
        "        )\n",
        "\n",
        "        return scores\n",
        "\n",
        "    def sentences_similarity(self, sentence_1, sentence_2):\n",
        "        tf_1 = Counter(self.tokenize_sentence(sentence_1))\n",
        "        tf_2 = Counter(self.tokenize_sentence(sentence_2))\n",
        "\n",
        "        similarity = self._idf_modified_cosine([tf_1, tf_2], 0, 1)\n",
        "\n",
        "        return similarity\n",
        "\n",
        "    def tokenize_sentence(self, sentence):\n",
        "        tokens = tokenize(\n",
        "            sentence,\n",
        "            self.stopwords,\n",
        "            keep_numbers=self.keep_numbers,\n",
        "            keep_emails=self.keep_emails,\n",
        "            keep_urls=self.keep_urls,\n",
        "        )\n",
        "\n",
        "        return tokens\n",
        "\n",
        "    def _calculate_idf(self, documents):\n",
        "        bags_of_words = []\n",
        "\n",
        "        for doc in documents:\n",
        "            doc_words = set()\n",
        "\n",
        "            for sentence in doc:\n",
        "                words = self.tokenize_sentence(sentence)\n",
        "                doc_words.update(words)\n",
        "\n",
        "            if doc_words:\n",
        "                bags_of_words.append(doc_words)\n",
        "\n",
        "        if not bags_of_words:\n",
        "            raise ValueError('documents are not informative')\n",
        "\n",
        "        doc_number_total = len(bags_of_words)\n",
        "\n",
        "        if self.include_new_words:\n",
        "            default_value = 1\n",
        "\n",
        "        else:\n",
        "            default_value = 0\n",
        "\n",
        "        idf_score = defaultdict(lambda: default_value)\n",
        "\n",
        "        for word in set.union(*bags_of_words):\n",
        "            doc_number_word = sum(1 for bag in bags_of_words if word in bag)\n",
        "            idf_score[word] = math.log(doc_number_total / doc_number_word)\n",
        "\n",
        "        return idf_score\n",
        "\n",
        "    def _calculate_similarity_matrix(self, tf_scores):\n",
        "        length = len(tf_scores)\n",
        "\n",
        "        similarity_matrix = np.zeros([length] * 2)\n",
        "\n",
        "        for i in range(length):\n",
        "            for j in range(i, length):\n",
        "                similarity = self._idf_modified_cosine(tf_scores, i, j)\n",
        "\n",
        "                if similarity:\n",
        "                    similarity_matrix[i, j] = similarity\n",
        "                    similarity_matrix[j, i] = similarity\n",
        "\n",
        "        return similarity_matrix\n",
        "\n",
        "    def _idf_modified_cosine(self, tf_scores, i, j):\n",
        "        if i == j:\n",
        "            return 1\n",
        "\n",
        "        tf_i, tf_j = tf_scores[i], tf_scores[j]\n",
        "        words_i, words_j = set(tf_i.keys()), set(tf_j.keys())\n",
        "\n",
        "        nominator = 0\n",
        "\n",
        "        for word in words_i & words_j:\n",
        "            idf = self.idf_score[word]\n",
        "            nominator += tf_i[word] * tf_j[word] * idf ** 2\n",
        "\n",
        "        if math.isclose(nominator, 0):\n",
        "            return 0\n",
        "\n",
        "        denominator_i, denominator_j = 0, 0\n",
        "\n",
        "        for word in words_i:\n",
        "            tfidf = tf_i[word] * self.idf_score[word]\n",
        "            denominator_i += tfidf ** 2\n",
        "\n",
        "        for word in words_j:\n",
        "            tfidf = tf_j[word] * self.idf_score[word]\n",
        "            denominator_j += tfidf ** 2\n",
        "\n",
        "        similarity = nominator / math.sqrt(denominator_i * denominator_j)\n",
        "\n",
        "        return similarity\n",
        "\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lxr = LexRank(train_article_set[:20000], stopwords=stop_words)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "QRyvUC0FooFa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Calculate rouge and bleu metrics for validation set of articles\n",
        "predicted_summaries = []\n",
        "\n",
        "for text in validation_article_set[:10000]:\n",
        "  summary = lxr.get_summary(text.split(\". \"), summary_size=2, threshold=.1)\n",
        "  summary = '. '.join(summary)\n",
        "  predicted_summaries.append(summary)\n",
        "  #print(validation_article_set[0])\n",
        "  #print(summary)\n",
        "\n",
        "# Compute the ROUGE metrics\n",
        "rouge = Rouge()\n",
        "rouge_scores = rouge.get_scores(predicted_summaries, validation_highlights_set[:10000], avg=True)\n",
        "\n",
        "print(\"ROUGE scores:\")\n",
        "print(rouge_scores)\n",
        "\n",
        "score_1 = round(rouge_scores['rouge-1']['f'], 2)    \n",
        "score_2 = round(rouge_scores['rouge-2']['f'], 2)    \n",
        "score_L = round(rouge_scores['rouge-l']['f'], 2)    \n",
        "print(\"rouge1:\", score_1, \"| rouge2:\", score_2, \"| rougeL:\",\n",
        "         score_2, \"--> avg rouge:\", round(np.mean(\n",
        "         [score_1,score_2,score_L]), 2))\n",
        "\n",
        "# Compute the BLEU metrics\n",
        "bleu_scores = corpus_bleu([[summary] for summary in predicted_summaries], validation_highlights_set[:10000])\n",
        "\n",
        "print(\"BLEU score:\")\n",
        "print(bleu_scores)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TKNEpZLeowLL",
        "outputId": "f7df1a1d-a796-45b9-e214-6ab35cb31705"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROUGE scores:\n",
            "{'rouge-1': {'r': 0.2945798196655688, 'p': 0.3151642471394741, 'f': 0.28978638281526614}, 'rouge-2': {'r': 0.09879655918793674, 'p': 0.10420366552923813, 'f': 0.09563913235156897}, 'rouge-l': {'r': 0.26681010395950344, 'p': 0.2863445242775333, 'f': 0.2628201212308664}}\n",
            "rouge1: 0.29 | rouge2: 0.1 | rougeL: 0.1 --> avg rouge: 0.22\n",
            "BLEU score:\n",
            "0.41883299293731435\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "p_summaries = []\n",
        "\n",
        "for text in test_article_set:\n",
        "  summary = lxr.get_summary(text.split(\". \"), summary_size=2, threshold=.1)\n",
        "  summary = '. '.join(summary)\n",
        "  p_summaries.append(summary)\n",
        "  #print(validation_article_set[0])\n",
        "  #print(summary)\n",
        "\n",
        "# Compute the ROUGE metrics\n",
        "rouge = Rouge()\n",
        "rouge_scores = rouge.get_scores(p_summaries, test_highlights_set, avg=True)\n",
        "\n",
        "print(\"ROUGE scores:\")\n",
        "print(rouge_scores)\n",
        "\n",
        "score_1 = round(rouge_scores['rouge-1']['f'], 2)    \n",
        "score_2 = round(rouge_scores['rouge-2']['f'], 2)    \n",
        "score_L = round(rouge_scores['rouge-l']['f'], 2)    \n",
        "print(\"rouge1:\", score_1, \"| rouge2:\", score_2, \"| rougeL:\",\n",
        "         score_2, \"--> avg rouge:\", round(np.mean(\n",
        "         [score_1,score_2,score_L]), 2))\n",
        "\n",
        "# Compute the BLEU metrics\n",
        "bleu_scores = corpus_bleu([[summary] for summary in p_summaries], test_highlights_set)\n",
        "\n",
        "print(\"BLEU score:\")\n",
        "print(bleu_scores)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BF6grSGOuC1M",
        "outputId": "be81c4b4-3a76-415d-ecb6-fddaa0f5833a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROUGE scores:\n",
            "{'rouge-1': {'r': 0.2964326842090193, 'p': 0.30982900982098954, 'f': 0.28760197181796165}, 'rouge-2': {'r': 0.09893378444815194, 'p': 0.10210414020021089, 'f': 0.09439111229852885}, 'rouge-l': {'r': 0.26810634006438644, 'p': 0.2812412837900786, 'f': 0.2605507773980212}}\n",
            "rouge1: 0.29 | rouge2: 0.09 | rougeL: 0.09 --> avg rouge: 0.21\n",
            "BLEU score:\n",
            "0.42031074855816086\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}