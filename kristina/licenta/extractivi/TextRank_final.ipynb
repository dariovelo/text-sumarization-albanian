{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets\n",
        "!pip install rouge\n",
        "!pip install nltk\n",
        "!pip install sumy\n",
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip glove*.zip"
      ],
      "metadata": {
        "id": "QcyCDCExPb8-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import list_datasets\n",
        "from datasets import load_dataset\n",
        "\n",
        "\n",
        "dataset = load_dataset('cnn_dailymail', '3.0.0')\n",
        "\n",
        "# split data in train, validation, test\n",
        "train_article_set = dataset['train']['article']\n",
        "train_highlights_set = dataset['train']['highlights']\n",
        "\n",
        "validation_article_set = dataset['validation']['article']\n",
        "validation_highlights_set = dataset['validation']['highlights']\n",
        "\n",
        "test_article_set = dataset['test']['article']\n",
        "test_highlights_set = dataset['test']['highlights']"
      ],
      "metadata": {
        "id": "4Hc7AGU5PmP3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TextRank folosind sumy"
      ],
      "metadata": {
        "id": "hPjdV6lK8LH7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RyMtGZd1PLbn"
      },
      "outputs": [],
      "source": [
        "from sumy.parsers.plaintext import PlaintextParser\n",
        "from sumy.nlp.tokenizers import Tokenizer\n",
        "from sumy.summarizers.text_rank import TextRankSummarizer\n",
        "import nltk\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "nltk.download('punkt')\n",
        "predicted_summary = []\n",
        "\n",
        "for text in validation_article_set:\n",
        "  # Create a parser for the text\n",
        "  parser = PlaintextParser.from_string(text, Tokenizer(\"english\"))\n",
        "\n",
        "  # Initialize the TextRank summarizer\n",
        "  summarizer = TextRankSummarizer()\n",
        "\n",
        "  # Set the number of sentences in the summary\n",
        "  summary_length = 1\n",
        "\n",
        "  # Generate the summary\n",
        "  summary = summarizer(parser.document, summary_length)\n",
        "\n",
        "  # Print the summary\n",
        "  s = \"\"\n",
        "  for sentence in summary:\n",
        "    s = s + str(sentence)\n",
        "  predicted_summary.append(s)\n",
        "\n",
        "\n",
        "from rouge import Rouge\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "\n",
        "# Compute the ROUGE metrics\n",
        "rouge = Rouge()\n",
        "rouge_scores = rouge.get_scores(predicted_summary, validation_highlights_set, avg=True)\n",
        "\n",
        "print(\"ROUGE scores:\")\n",
        "print(rouge_scores)\n",
        "score_1 = round(rouge_scores['rouge-1']['f'], 2)    \n",
        "score_2 = round(rouge_scores['rouge-2']['f'], 2)    \n",
        "score_L = round(rouge_scores['rouge-l']['f'], 2)    \n",
        "print(\"rouge1:\", score_1, \"| rouge2:\", score_2, \"| rougeL:\",\n",
        "         score_2, \"--> avg rouge:\", round(np.mean(\n",
        "         [score_1,score_2,score_L]), 2))\n",
        "\n",
        "# Compute the BLEU metrics\n",
        "bleu_scores = corpus_bleu([[summary] for summary in predicted_summary], validation_highlights_set)\n",
        "\n",
        "print(\"BLEU score:\")\n",
        "print(bleu_scores)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TextRank using cosine similarity"
      ],
      "metadata": {
        "id": "xEDqtmcATQLU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "nltk.download('punkt') \n",
        "import re"
      ],
      "metadata": {
        "id": "gjNgVFhC30Fc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract word vectors\n",
        "word_embeddings = {}\n",
        "f = open('glove.6B.100d.txt', encoding='utf-8')\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    word_embeddings[word] = coefs\n",
        "f.close()\n",
        "len(word_embeddings)"
      ],
      "metadata": {
        "id": "lkhj737D4Zm5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "def preprocess_text(article):\n",
        "  # remove punctuations, numbers and special characters\n",
        "  sentences = sent_tokenize(article)\n",
        "\n",
        "  clean_sentences = pd.Series(sentences).str.replace(\"[^a-zA-Z]\", \" \")\n",
        "\n",
        "  # make alphabets lowercase\n",
        "  clean_sentences = [s.lower() for s in clean_sentences]\n",
        "\n",
        "  return clean_sentences"
      ],
      "metadata": {
        "id": "0K_2ke3K4ew5"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = stopwords.words('english')\n",
        "# function to remove stopwords\n",
        "def remove_stopwords(sen):\n",
        "    sen_new = \" \".join([i for i in sen if i not in stop_words])\n",
        "    return sen_new\n"
      ],
      "metadata": {
        "id": "adFfZMl14iuq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract word vectors\n",
        "word_embeddings = {}\n",
        "f = open('glove.6B.100d.txt', encoding='utf-8')\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    word_embeddings[word] = coefs\n",
        "f.close()"
      ],
      "metadata": {
        "id": "_oMQ-RMZ42YC"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def create_vectors(clean_sentences):\n",
        "  # remove stopwords from the sentences\n",
        "  clean_sentences = [remove_stopwords(r.split()) for r in clean_sentences]\n",
        "  sentence_vectors = []\n",
        "  for i in clean_sentences:\n",
        "    if len(i) != 0:\n",
        "      v = sum([word_embeddings.get(w, np.zeros((100,))) for w in i.split()])/(len(i.split())+0.001)\n",
        "    else:\n",
        "      v = np.zeros((100,))\n",
        "    sentence_vectors.append(v)\n",
        "  \n",
        "  return sentence_vectors"
      ],
      "metadata": {
        "id": "EjRG1Zqv45IE"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import networkx as nx\n",
        "\n",
        "def rank_sentences(article):\n",
        "  clean_sentences = preprocess_text(article)\n",
        "  sentence_vectors = create_vectors(clean_sentences)\n",
        "  # similarity matrix\n",
        "  sim_mat = np.zeros([len(clean_sentences), len(clean_sentences)])\n",
        "  for i in range(len(clean_sentences)):\n",
        "    for j in range(len(clean_sentences)):\n",
        "      if i != j:\n",
        "        sim_mat[i][j] = cosine_similarity(sentence_vectors[i].reshape(1,100), sentence_vectors[j].reshape(1,100))[0,0]\n",
        "  \n",
        "  nx_graph = nx.from_numpy_array(sim_mat)\n",
        "  scores = nx.pagerank(nx_graph)\n",
        "\n",
        "  ranked_sentences = sorted(((scores[i],s) for i,s in enumerate(clean_sentences)), reverse=True)\n",
        "  return ranked_sentences\n",
        "  \n",
        "\n"
      ],
      "metadata": {
        "id": "zzcrnGb55SAL"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_summaries = []\n",
        "\n",
        "for text in train_article_set:\n",
        "\n",
        "  ranked_sentences = rank_sentences(text)\n",
        "  sn = 2\n",
        "  summary = \"\"\n",
        "  # Generate summary\n",
        "  if len(ranked_sentences) >= sn:\n",
        "    for i in range(sn):\n",
        "      summary = summary + ranked_sentences[i][1]\n",
        "    predicted_summaries.append(summary)\n",
        "  else:\n",
        "    predicted_summaries.append(ranked_sentences[0][1])\n"
      ],
      "metadata": {
        "id": "F5aQJb1X5anB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from rouge import Rouge\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "\n",
        "# Compute the ROUGE metrics\n",
        "rouge = Rouge()\n",
        "\n",
        "rouge_scores = rouge.get_scores(predicted_summaries, train_highlights_set, avg=True)\n",
        "\n",
        "print(\"ROUGE scores:\")\n",
        "print(rouge_scores)\n",
        "\n",
        "score_1 = round(rouge_scores['rouge-1']['f'], 2)    \n",
        "score_2 = round(rouge_scores['rouge-2']['f'], 2)    \n",
        "score_L = round(rouge_scores['rouge-l']['f'], 2)    \n",
        "print(\"rouge1:\", score_1, \"| rouge2:\", score_2, \"| rougeL:\",\n",
        "         score_2, \"--> avg rouge:\", round(np.mean(\n",
        "         [score_1,score_2,score_L]), 2))\n",
        "\n",
        "# Compute the BLEU metrics\n",
        "bleu_scores = corpus_bleu([[summary] for summary in predicted_summaries], train_highlights_set)\n",
        "\n",
        "print(\"BLEU score:\")\n",
        "print(bleu_scores)"
      ],
      "metadata": {
        "id": "QLS2EogI-HFi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}