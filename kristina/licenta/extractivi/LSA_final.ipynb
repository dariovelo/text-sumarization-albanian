{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "class ItemsCount(object):\n",
        "    def __init__(self, value):\n",
        "        self._value = value\n",
        "\n",
        "    def __call__(self, sequence):\n",
        "        if isinstance(self._value, (bytes, str,)):\n",
        "            if self._value.endswith(\"%\"):\n",
        "                total_count = len(sequence)\n",
        "                percentage = int(self._value[:-1])\n",
        "                # at least one sentence should be chosen\n",
        "                count = max(1, total_count*percentage // 100)\n",
        "                return sequence[:count]\n",
        "            else:\n",
        "                return sequence[:int(self._value)]\n",
        "        elif isinstance(self._value, (int, float)):\n",
        "            return sequence[:int(self._value)]\n",
        "        else:\n",
        "            ValueError(\"Unsuported value of items count '%s'.\" % self._value)\n",
        "\n",
        "    def __repr__(self):\n",
        "        return to_string(\"<ItemsCount: %r>\" % self._value)\n"
      ],
      "metadata": {
        "id": "TZUt9t9sfpaW"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from operator import attrgetter\n",
        "from collections import namedtuple\n",
        "\n",
        "SentenceInfo = namedtuple(\"SentenceInfo\", (\"sentence\", \"order\", \"rating\",))\n",
        "\n",
        "class BaseSummarizer(object):\n",
        "    \n",
        "    def __call__(self, document, sentences_count):\n",
        "        raise NotImplementedError(\"This method should be overriden in subclass\")\n",
        "\n",
        "    @staticmethod\n",
        "    def normalize_word(word):\n",
        "        return word.lower()\n",
        "\n",
        "    @staticmethod\n",
        "    def _get_best_sentences(sentences, count, rating, *args, **kwargs):\n",
        "        rate = rating\n",
        "        if isinstance(rating, dict):\n",
        "            assert not args and not kwargs\n",
        "            rate = lambda s: rating[s]\n",
        "\n",
        "        infos = (SentenceInfo(s, o, rate(s, *args, **kwargs))\n",
        "            for o, s in enumerate(sentences))\n",
        "\n",
        "        # sort sentences by rating in descending order\n",
        "        infos = sorted(infos, key=attrgetter(\"rating\"), reverse=True)\n",
        "        # get `count` first best rated sentences\n",
        "        if not isinstance(count, ItemsCount):\n",
        "            count = ItemsCount(count)\n",
        "        infos = count(infos)\n",
        "        # sort sentences by their order in document\n",
        "        infos = sorted(infos, key=attrgetter(\"order\"))\n",
        "\n",
        "        return tuple(i.sentence for i in infos)"
      ],
      "metadata": {
        "id": "ZC-CaEksfqZ5"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division, print_function, unicode_literals\n",
        "import math\n",
        "import numpy\n",
        "import nltk\n",
        "\n",
        "from warnings import warn\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from numpy.linalg import svd as singular_value_decomposition\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "class LsaSummarizer(BaseSummarizer):\n",
        "    MIN_DIMENSIONS = 3\n",
        "    REDUCTION_RATIO = 1/1\n",
        "\n",
        "    _stop_words = list(stopwords.words('english'))\n",
        "\n",
        "    @property\n",
        "    def stop_words(self):\n",
        "        return self._stop_words\n",
        "\n",
        "    @stop_words.setter\n",
        "    def stop_words(self, words):\n",
        "        self._stop_words = words\n",
        "\n",
        "    def __call__(self, document, sentences_count):\n",
        "\n",
        "        dictionary = self._create_dictionary(document)\n",
        "        \n",
        "        if not dictionary:\n",
        "            return ()\n",
        "\n",
        "        sentences = sent_tokenize(document)\n",
        "\n",
        "        matrix = self._create_matrix(document, dictionary)\n",
        "        matrix = self._compute_term_frequency(matrix)\n",
        "        u, sigma, v = singular_value_decomposition(matrix, full_matrices=False)\n",
        "\n",
        "        ranks = iter(self._compute_ranks(sigma, v))\n",
        "        return self._get_best_sentences(sentences, sentences_count,\n",
        "            lambda s: next(ranks))\n",
        "\n",
        "    def _create_dictionary(self, document):\n",
        "        \"\"\"Creates mapping key = word, value = row index\"\"\"\n",
        "\n",
        "        words = word_tokenize(document)\n",
        "        words = tuple(words)\n",
        "\n",
        "        words = map(self.normalize_word, words)\n",
        "\n",
        "        unique_words = frozenset(w for w in words if w not in self._stop_words)\n",
        "\n",
        "        return dict((w, i) for i, w in enumerate(unique_words))\n",
        "\n",
        "    def _create_matrix(self, document, dictionary):\n",
        "        \"\"\"\n",
        "        Creates matrix of shape where cells\n",
        "        contains number of occurences of words (rows) in senteces (cols).\n",
        "        \"\"\"\n",
        "        sentences = sent_tokenize(document)\n",
        "        words_count = len(dictionary)\n",
        "        sentences_count = len(sentences)\n",
        "        if words_count < sentences_count:\n",
        "            message = (\n",
        "                \"Number of words (%d) is lower than number of sentences (%d). \"\n",
        "                \"LSA algorithm may not work properly.\"\n",
        "            )\n",
        "            warn(message % (words_count, sentences_count))\n",
        "\n",
        "        matrix = numpy.zeros((words_count, sentences_count))\n",
        "        for col, sentence in enumerate(sentences):\n",
        "            words = word_tokenize(sentence)\n",
        "            for word in words:\n",
        "                # only valid words is counted (not stop-words, ...)\n",
        "                if word in dictionary:\n",
        "                    row = dictionary[word]\n",
        "                    matrix[row, col] += 1\n",
        "\n",
        "        return matrix\n",
        "\n",
        "    def _compute_term_frequency(self, matrix, smooth=0.4):\n",
        "        \"\"\"\n",
        "        Computes TF metrics for each sentence (column) in the given matrix and  normalize \n",
        "        the tf weights of all terms occurring in a document by the maximum tf in that document \n",
        "        according to ntf_{t,d} = a + (1-a)\\frac{tf_{t,d}}{tf_{max}(d)^{'}}.\n",
        "        \n",
        "        The smoothing term $a$ damps the contribution of the second term - which may be viewed \n",
        "        as a scaling down of tf by the largest tf value in $d$\n",
        "        \"\"\"\n",
        "        assert 0.0 <= smooth < 1.0\n",
        "\n",
        "        max_word_frequencies = numpy.max(matrix, axis=0)\n",
        "        rows, cols = matrix.shape\n",
        "        for row in range(rows):\n",
        "            for col in range(cols):\n",
        "                max_word_frequency = max_word_frequencies[col]\n",
        "                if max_word_frequency != 0:\n",
        "                    frequency = matrix[row, col]/max_word_frequency\n",
        "                    matrix[row, col] = smooth + (1.0 - smooth)*frequency\n",
        "\n",
        "        return matrix\n",
        "\n",
        "    def _compute_ranks(self, sigma, v_matrix):\n",
        "        assert len(sigma) == v_matrix.shape[0]\n",
        "\n",
        "        dimensions = max(LsaSummarizer.MIN_DIMENSIONS,\n",
        "            int(len(sigma)*LsaSummarizer.REDUCTION_RATIO))\n",
        "        powered_sigma = tuple(s**2 if i < dimensions else 0.0\n",
        "            for i, s in enumerate(sigma))\n",
        "\n",
        "        ranks = []\n",
        "        \n",
        "        for column_vector in v_matrix.T:\n",
        "            rank = sum(s*v**2 for s, v in zip(powered_sigma, column_vector))\n",
        "            ranks.append(math.sqrt(rank))\n",
        "\n",
        "        return ranks"
      ],
      "metadata": {
        "id": "Q-IEYR5kf8vP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets\n",
        "!pip install rouge\n",
        "!pip install nltk"
      ],
      "metadata": {
        "id": "BunamGOSg56b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import list_datasets\n",
        "from datasets import load_dataset\n",
        "\n",
        "\n",
        "print(list_datasets())\n",
        "dataset = load_dataset('cnn_dailymail', '3.0.0')\n",
        "\n",
        "# split data in train, validation, test\n",
        "train_article_set = dataset['train']['article']\n",
        "train_highlights_set = dataset['train']['highlights']\n",
        "\n",
        "validation_article_set = dataset['validation']['article']\n",
        "validation_highlights_set = dataset['validation']['highlights']\n",
        "\n",
        "test_article_set = dataset['test']['article']\n",
        "test_highlights_set = dataset['test']['highlights']"
      ],
      "metadata": {
        "id": "rRPozTXHhD-i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summarizer = LsaSummarizer()\n",
        "predicted_summary = []\n",
        "for article in train_article_set[:50000]:\n",
        "  summary = summarizer(article, 2)\n",
        "  summ = \"\"\n",
        "  for s in summary:\n",
        "    summ += s\n",
        "  predicted_summary.append(summ)\n",
        "\n"
      ],
      "metadata": {
        "id": "jUBXSiJDhQeJ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from rouge import Rouge\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "import numpy as np\n",
        "\n",
        "# Compute the ROUGE metrics\n",
        "rouge = Rouge()\n",
        "rouge_scores = rouge.get_scores(predicted_summary, train_highlights_set[:50000], avg=True)\n",
        "\n",
        "print(\"ROUGE scores:\")\n",
        "print(rouge_scores)\n",
        "\n",
        "score_1 = round(rouge_scores['rouge-1']['f'], 2)    \n",
        "score_2 = round(rouge_scores['rouge-2']['f'], 2)    \n",
        "score_L = round(rouge_scores['rouge-l']['f'], 2)    \n",
        "print(\"rouge1:\", score_1, \"| rouge2:\", score_2, \"| rougeL:\",\n",
        "         score_2, \"--> avg rouge:\", round(np.mean(\n",
        "         [score_1,score_2,score_L]), 2))\n",
        "\n",
        "# Compute the BLEU metrics\n",
        "bleu_scores = corpus_bleu([[summary] for summary in predicted_summary], train_highlights_set[:50000])\n",
        "\n",
        "print(\"BLEU score:\")\n",
        "print(bleu_scores)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zLd8xX9khQ5T",
        "outputId": "d7f2581d-9d18-4b9c-c719-be026e645788"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROUGE scores:\n",
            "{'rouge-1': {'r': 0.2528294307378787, 'p': 0.19353671066755967, 'f': 0.21585414062381064}, 'rouge-2': {'r': 0.06467345573681514, 'p': 0.047723549775064156, 'f': 0.05383784931379033}, 'rouge-l': {'r': 0.22789444270301173, 'p': 0.17450668726013013, 'f': 0.19458683013098216}}\n",
            "rouge1: 0.22 | rouge2: 0.05 | rougeL: 0.05 --> avg rouge: 0.15\n",
            "BLEU score:\n",
            "0.32651672727047265\n"
          ]
        }
      ]
    }
  ]
}